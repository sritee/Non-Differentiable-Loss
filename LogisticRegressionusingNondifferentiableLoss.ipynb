{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Dealing with  non-differentiable loss functions in ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let the loss function $$f(y)=\\mathbb1(h_\\theta(x)\\ne y)$$\n",
    "\n",
    "Clearly, it is non differentiable with respect to the weights.\n",
    "Hence, we resort to policy gradient methods, which is used in reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective is to minimize the loss f(y), i.e $$\\min E_{y \\sim P_{\\theta}} \\left[ f(y) \\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\\nabla_{\\theta} E_{y \\sim P_{\\theta}}[ f(y)] = 
    "$$=  E_{y \\sim P_{\\theta}} \\left[ f(y)  \\ln \\nabla_{\\theta} P_{\\theta}(y) \\right]$$\n",
    "\n",
    "Note that now, we are no longer directly differentiating the loss. This manipulations is sometimes called the log derivative trick\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "Let us dive into the code. We will create a synthetic data set to test whether it works, where\n",
       "$$y=\\begin{cases} \n",
       "      1  &  x> 0.5 \\\\\n",
       "      \n",
       "      0 & x \\leq 0.5\n",
       "   \\end{cases}\n",
       "$$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "Let us dive into the code. We will create a synthetic data set to test whether it works, where\n",
    "$$y=\\begin{cases} \n",
    "      1  &  x> 0.5 \\\\\n",
    "      \n",
    "      0 & x \\leq 0.5\n",
    "   \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=np.random.rand(200,1)\n",
    "y=(x>0.5).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setting up predictor function\n",
    "def sigmoid(z,weights):\n",
    "    temp=1+np.exp(-(weights[0]*z+weights[1]))\n",
    "    return 1.0/temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma(x)= 1/(1+exp(-x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "Accuracy is 0.94\n"
     ]
    }
   ],
   "source": [
    "alpha=0.1\n",
    "weights=np.random.rand(2,1)*0.01\n",
    "#print(weights)\n",
    "num_iter=1000\n",
    "minibatch_size=8\n",
    "\n",
    "for h in range(num_iter):\n",
    "    change=0\n",
    "    minibatch=np.array(random.sample(list(np.hstack([x,y])),minibatch_size))\n",
    "    x_samp=minibatch[:,0]\n",
    "    y_samp=minibatch[:,1]\n",
    "    #print(x_samp)\n",
    "    #print(y_samp)\n",
    "    for m,l in enumerate(list(x_samp)):\n",
    "        \n",
    "        prob_positive=sigmoid(l,weights) # computing probability of class 1\n",
    "        #print(logpred)\n",
    "        ypred=np.random.rand()<prob_positive # sampling from binomial disto\n",
    "        \n",
    "        reward= 2*(ypred==y_samp[m])-1 #reward setting\n",
    "       \n",
    "        if ypred==1:\n",
    "            grads=np.array([(1-prob_positive)*l,(1-prob_positive)])\n",
    "        \n",
    "        if ypred==0:\n",
    "            grads=np.array([-prob_positive*l,-prob_positive])\n",
    "    \n",
    "        change=change+alpha*grads*reward\n",
    "        #print(grads)\n",
    "    #print(change/minibatch_size)\n",
    "    weights=weights+change/minibatch_size\n",
    "    if h%200==0:\n",
    "        print(h)\n",
    "        \n",
    "#checking the accuracy        \n",
    "pred=[]\n",
    "\n",
    "for idx,el in enumerate(x):\n",
    "    \n",
    "    positive_prob=sigmoid(el,weights)\n",
    "    \n",
    "    if positive_prob>=0.5:\n",
    "        t=1\n",
    "    else:\n",
    "        t=0\n",
    "    pred.append(t)\n",
    "print('Accuracy is {}'.format(accuracy_score(pred,y)))\n",
    "#print(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
